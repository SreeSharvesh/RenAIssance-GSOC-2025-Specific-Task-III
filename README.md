# Synthetic Renaissance Text Generation with Generative Models

## Overview
This project aims to generate synthetic Renaissance-style printed text with realistic imperfections such as ink bleed, smudging, and faded text using a generative model. The goal is to use historical Spanish text (from the 17th century) and generate images for at least 5 pages with visible degradation effects. The model is based on a **Generative Adversarial Network (GAN)** architecture, combining a **U-Net-based Generator** and a **PatchGAN Discriminator** to produce high-quality, degraded text images.

## Task and Approach

The task involves designing a mid-scale generative model to create synthetic Renaissance-style printed text. The key challenge is to replicate historical printing imperfections accurately. The chosen approach uses a **GAN-based model** with a U-Net architecture for image generation and a PatchGAN discriminator for distinguishing real from generated images. The process introduces realistic printing artifacts through data augmentation techniques such as ink bleed, smudging, and fading.

### Dataset Preparation
A dataset of clean and degraded images is created. The clean images are generated from historical text, while the degraded images are augmented with imperfections to simulate Renaissance-style printing. The following steps were used in dataset creation:

1. **Clean Image Creation**: The clean images are generated by rendering the text in a Renaissance-style font (EB Garamond) onto a 512x512 grayscale image.
2. **Degraded Image Augmentation**: The clean images are then augmented with the following degradation effects:
   - Ink bleed using Gaussian blur
   - Smudging with affine transformations
   - Fading using alpha blending
   - Noise addition to simulate historical imperfections

The dataset consists of **100 pairs** of clean and degraded images (5 pages x 20 variations per page).

### Model Architecture

1. **Generator**: The generator model is based on a U-Net architecture, which allows for high-resolution image generation with skip connections between the encoder and decoder. The generator learns to transform clean text images into degraded ones by learning to introduce printing imperfections.

2. **Discriminator**: The discriminator uses a PatchGAN architecture to classify each patch of the image as real or fake. It takes both the clean and the generated (degraded) images as input and learns to distinguish between them.

### Training
The training process consists of alternating between training the generator and the discriminator:
- **Discriminator Training**: The discriminator is trained to classify real and fake image pairs.
- **Generator Training**: The generator is trained to fool the discriminator while minimizing the L1 loss between the generated image and the degraded image.

### Evaluation Metrics
#### 1. Structural Similarity Index (SSIM):
Description: SSIM is a perceptual metric that measures the similarity between two images by considering changes in structural information, luminance, and texture. It is commonly used in image processing tasks and can be useful for comparing your generated degraded images with real samples of historical texts.

Usage: It is a widely accepted method to evaluate image quality and is sensitive to small variations that can represent imperfections like ink bleed, smudging, or fading.

#### 2. Peak Signal-to-Noise Ratio (PSNR):
Description: PSNR is a metric that quantifies the quality of an image by comparing the difference between the original and the distorted image. Higher values of PSNR indicate less distortion.

Usage: PSNR can be used to quantify how much noise or degradation has been introduced in the generated images, especially when comparing them to clean ground truth samples.

####  3. Fr√©chet Inception Distance (FID):
Description: FID is a metric that compares the distribution of generated images to that of real images. It uses feature vectors from a pretrained Inception network to capture the similarities between the two distributions.

Usage: FID is commonly used in evaluating generative models like GANs and can be adapted to measure how similar the generated text images are to real historical text images in terms of overall visual characteristics.

#### 4. Cosine Similarity (for Text-based Features):
Description: If the generated text images contain readable text (e.g., OCR outputs), we can use text similarity metrics such as cosine similarity to compare the content of the generated text to the original historical text.

Usage: An OCR model can be used to extract the text from the generated and real samples, and then compute cosine similarity between their vector representations. This can help evaluate the textual fidelity of the generated content.

### Current Status and Next Steps
I have completed Task 2. Currently, I am working on finetuning this model, which involves training the GAN model and generating synthetic renaissance style text from the images

Given that the deadline is approaching, I am submitting my progress as is because I believe this approach is worth pursuing and is a solid direction to take. I am confident that with the current methodology, the remaining tasks will be completed in a timely manner. 

The GAN architecture and dataset have shown promising potential, and I am optimistic about achieving a high-quality result once the model is fully trained and evaluated.

In the next steps, I will proceed with the following:

1. Train the GAN model using the prepared dataset.

2. Generate synthetic degraded images from clean text.

3. Evaluate the performance using SSIM and other metrics.

Here are the evaluation metrics of my current progress:

| Epoch | Step  | D Loss  | G Loss  |
|-------|-------|---------|---------|
| 2     | 50    | 0.3617  | 16.8972 |
| 4     | 50    | 0.3736  | 17.1358 |
| 6     | 50    | 0.3638  | 23.3622 |
| 8     | 50    | 0.8604  | 8.7741  |
| 10    | 50    | 0.4230  | 23.2497 |

More can be seen from the `.ipynb` file.

### Conclusion
This approach leverages GANs for generating synthetic Renaissance-style printed text with realistic degradation effects. By using a dataset of historical text and augmenting it with printing imperfections, the model aims to replicate the look of 17th-century printed pages. The next steps involve training the model, generating synthetic text images, and evaluating their quality.


